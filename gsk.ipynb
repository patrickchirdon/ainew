{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/anaconda3/envs/my-rdkit-env/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-443e0590d5c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mnodes1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mnodes2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mnodes3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from sklearn.externals import joblib\n",
    "# dense to sparse\n",
    "from numpy import array\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from keras import regularizers\n",
    "abscaler= preprocessing.MaxAbsScaler()\n",
    "from os import environ\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "f = open('csvfile.csv')\n",
    "csv_f = csv.reader(f)\n",
    "i=0\n",
    "args=[]\n",
    "for row in csv_f:\n",
    "    args+=row\n",
    "\n",
    "relu=np.int64(args[0])\n",
    "nodes1=np.int64(args[1])\n",
    "nodes2=np.int64(args[2])\n",
    "nodes3=np.int64(args[3])\n",
    "nodes4=np.int64(args[4])\n",
    "dropout=np.int64(args[5])\n",
    "dropout1=float(args[6])\n",
    "dropout2=float(args[7])\n",
    "dropout3=float(args[8])\n",
    "dropout4=float(args[9])\n",
    "regularizers1=float(args[10])\n",
    "regularizers2=float(args[11])\n",
    "regularizers3=float(args[12])\n",
    "regularizers4=float(args[13])\n",
    "lr1=float(args[14])\n",
    "lr2=float(args[15])\n",
    "lr3=float(args[16])\n",
    "lr4=float(args[17])\n",
    "momentum1=float(args[18])\n",
    "momentum2=float(args[19])\n",
    "momentum3=float(args[20])\n",
    "momentum4=float(args[21])\n",
    "decay1=float(args[22])\n",
    "decay2=float(args[23])\n",
    "decay3=float(args[24])\n",
    "decay4=float(args[25])\n",
    "mynesterov=args[26]\n",
    "theloss=args[27]\n",
    "epochs1=np.int64(args[28])\n",
    "epochs2=np.int64(args[29])\n",
    "epochs3=np.int64(args[30])\n",
    "epochs4=np.int64(args[31])\n",
    "batchsize1=np.int64(args[32])\n",
    "batchsize2=np.int64(args[33])\n",
    "batchsize3=np.int64(args[34])\n",
    "batchsize4=np.int64(args[35])\n",
    "filename=args[36]\n",
    "modelname1=args[37]\n",
    "modelname2=args[38]\n",
    "modelname3=args[39]\n",
    "modelname4=args[40]\n",
    "penalty1 = np.int64(args[41])\n",
    "penalty2 = np.int64(args[42])\n",
    "penalty3 = np.int64(args[43])\n",
    "penalty4 = np.int64(args[44])\n",
    "modelname1a=args[45]\n",
    "modelname2a=args[46]\n",
    "modelname3a=args[47]\n",
    "modelname4a=args[48]\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from sklearn.externals import joblib\n",
    "def process(penalty, dropout, regularizers, lr, momentum, decay, nesterov, myloss, epochs, batchsize, filename, modelname):\n",
    "    \n",
    "    os.remove('output.txt')\n",
    "    dataset = open(filename, \"r\" )\n",
    "    dataset = [ line.rstrip().split(\"\\t\") for line in dataset ][0:]\n",
    "    mols = [ Chem.MolFromSmiles( line[0][0] ) for line in dataset ]\n",
    "\n",
    "\n",
    "    u=0\n",
    "    indexy=[]\n",
    "    for y in mols:\n",
    "        if y is not None:\n",
    "            indexy.append(u)\n",
    "            u+=1\n",
    "        \n",
    "    goodmols=[mols[k] for k in indexy]\n",
    "    dataset = [ line.rstrip().split(\"\\t\") for line in dataset ][0:]\n",
    "\n",
    "    Y= ([x[1] for x in dataset])\n",
    "    myvalues=[Y[j] for j in indexy]\n",
    "    Y=myvalues\n",
    "   \n",
    "    Y = [float(i) for i in Y]\n",
    "    Y = np.asarray( Y )\n",
    "\n",
    "    trainmols, testmols, trainy, testy = train_test_split(goodmols, Y, test_size = 0.1, random_state=90  )\n",
    "\n",
    "# calc fingerprint\n",
    "\n",
    "    trainfps = [AllChem.GetMorganFingerprintAsBitVect(m, 2) for m in trainmols if m is not None ]\n",
    "    testfps = [AllChem.GetMorganFingerprintAsBitVect(m, 2) for m in testmols if m is not None]\n",
    "    u=0\n",
    "    indexy=[]\n",
    "    for y in trainfps:\n",
    "        if y is not None:\n",
    "            indexy.append(u)\n",
    "            u+=1\n",
    "    goodY=[trainy[k] for k in indexy]\n",
    "\n",
    "    np_fptrain=[]\n",
    "    for fp in trainfps:\n",
    "        arr=np.zeros((1,))\n",
    "        DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "        np_fptrain.append(arr)\n",
    "\n",
    "    np_fptest=[]\n",
    "    for fp in testfps:\n",
    "        arr=np.zeros((1,))\n",
    "        DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "        np_fptest.append(arr)   \n",
    "\n",
    "    cls = SVC( probability=True, C=penalty)\n",
    "    cls.fit( trainfps, goodY )\n",
    "    pkl_name=filename + \".pkl\"\n",
    "    joblib.dump(cls, pkl_name)\n",
    "    \n",
    "    \n",
    "dataset = open(filename, \"r\" )\n",
    "dataset = [ line.rstrip().split(\",\") for line in dataset ][0:]\n",
    "mols = [ Chem.MolFromSmiles( line[0] ) for line in dataset ]\n",
    "\n",
    "\n",
    "u=0\n",
    "indexy=[]\n",
    "for y in mols:\n",
    "    if y is not None:\n",
    "        indexy.append(u)\n",
    "        u+=1\n",
    "        \n",
    "goodmols=[mols[k] for k in indexy]\n",
    "    \n",
    "   \n",
    "\n",
    "Y=[line[1] for line in dataset]\n",
    "goody=[Y[k] for k in indexy]\n",
    "trainmols, testmols, trainy, testy = train_test_split(goodmols, goody, test_size = 0.1, random_state=90  )\n",
    "#trainmols, testmols, trainy, testy = train_test_split(goodmols, Y, test_size = 0.1, random_state=90  )\n",
    "trainfps = [AllChem.GetMorganFingerprintAsBitVect(m, 2) for m in goodmols if m is not None ]\n",
    "testfps = [AllChem.GetMorganFingerprintAsBitVect(m, 2) for m in testmols if m is not None]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "u=0\n",
    "indexy=[]\n",
    "for y in trainfps:\n",
    "    if y is not None:\n",
    "        indexy.append(u)\n",
    "        u+=1\n",
    "    \n",
    "newy=array([int(goody[k]) for k in indexy])\n",
    "print(len(newy))\n",
    "    \n",
    "np_fptrain=[]\n",
    "for fp in trainfps:\n",
    "    arr=np.zeros((1,))\n",
    "    DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "    np_fptrain.append(arr)\n",
    "\n",
    "np_fptest=[]\n",
    "for fp in testfps:\n",
    "    arr=np.zeros((1,))\n",
    "    DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "    np_fptest.append(arr)  \n",
    "    \n",
    "    \n",
    "a=csr_matrix(np_fptrain, dtype=np.int8).toarray()\n",
    "b=csr_matrix(np_fptest, dtype=np.int8).toarray()\n",
    "a=abscaler.fit_transform(a)\n",
    "b=abscaler.fit_transform(b)\n",
    "    \n",
    "print(len(a))\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "   \n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    " \n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import regularizers as WeightRegularizer\n",
    "from keras.optimizers import SGD\n",
    "#SKlearn for metrics and datasplits\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "#Matplotlib for plotting\n",
    "from matplotlib import pyplot as plt\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(nodes1, init='uniform', activation='relu', kernel_regularizer=keras.regularizers.l2(l=regularizers1)))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "if dropout==1:\n",
    "    model.add(layers.Dropout(dropout1))\n",
    "\n",
    "if relu==2:\n",
    "    model.add(layers.Dense(nodes1, activation='relu', kernel_regularizer=keras.regularizers.l2(l=regularizers1)))\n",
    "model.add(layers.Dense(1,  activation='sigmoid'))\n",
    "    \n",
    "    \n",
    "    \n",
    "sgd=SGD(lr1, momentum1, decay1, nesterov=mynesterov)\n",
    "model.compile(loss=theloss, optimizer=sgd, metrics=['accuracy'])\n",
    "model.fit(a, newy, nb_epoch=epochs1, batch_size=batchsize1)\n",
    "    \n",
    "themodel=modelname1 + '.h5'\n",
    "model.save(themodel)\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    \n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(nodes1, init='uniform', activation='relu', kernel_regularizer=keras.regularizers.l2(l=regularizers1)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(dropout1))\n",
    "model.add(layers.Dense(nodes1, activation='relu', kernel_regularizer=keras.regularizers.l2(l=regularizers1)))\n",
    "    \n",
    "model.add(layers.Dense(1,  activation='sigmoid'))\n",
    "    \n",
    "    \n",
    "    \n",
    "sgd=SGD(lr1, momentum1, decay1, nesterov=mynesterov)\n",
    "model.compile(loss=theloss, optimizer=sgd, metrics=['accuracy'])\n",
    "model.fit(a, newy, nb_epoch=epochs1, batch_size=batchsize1)\n",
    "    \n",
    "themodel=modelname2 + '.h5'\n",
    "model.save(themodel)\n",
    "\n",
    "    \n",
    "    \n",
    "#####################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(nodes1, init='uniform', activation='relu', kernel_regularizer=keras.regularizers.l2(l=regularizers1)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(dropout1))\n",
    "model.add(layers.Dense(nodes1, activation='relu', kernel_regularizer=keras.regularizers.l2(l=regularizers1)))\n",
    "    \n",
    "model.add(layers.Dense(1,  activation='sigmoid'))\n",
    "    \n",
    "    \n",
    "    \n",
    "sgd=SGD(lr1, momentum1, decay1, nesterov=mynesterov)\n",
    "model.compile(loss=theloss, optimizer=sgd, metrics=['accuracy'])\n",
    "model.fit(a, newy, nb_epoch=epochs1, batch_size=batchsize1)\n",
    "    \n",
    "themodel=modelname3 + '.h5'\n",
    "model.save(themodel)\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(nodes1, init='uniform', activation='relu', kernel_regularizer=keras.regularizers.l2(l=regularizers1)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(dropout1))\n",
    "model.add(layers.Dense(nodes1, activation='relu', kernel_regularizer=keras.regularizers.l2(l=regularizers1)))\n",
    "    \n",
    "model.add(layers.Dense(1,  activation='sigmoid'))\n",
    "    \n",
    "    \n",
    "    \n",
    "sgd=SGD(lr1, momentum1, decay1, nesterov=mynesterov)\n",
    "model.compile(loss=theloss, optimizer=sgd, metrics=['accuracy'])\n",
    "model.fit(a, newy, nb_epoch=epochs1, batch_size=batchsize1)\n",
    "    \n",
    "themodel=modelname4 + '.h5'\n",
    "model.save(themodel)\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "cls = SVC( probability=True, C=penalty1)\n",
    "cls.fit( a, newy)\n",
    "pkl_name=modelname1a + \".pkl\"\n",
    "joblib.dump(cls, pkl_name)\n",
    "    \n",
    "cls = SVC( probability=True, C=penalty2)\n",
    "cls.fit(a, newy)\n",
    "pkl_name=modelname2a + \".pkl\"\n",
    "joblib.dump(cls, pkl_name)\n",
    "    \n",
    "cls = SVC( probability=True, C=penalty3)\n",
    "cls.fit(a, newy)\n",
    "pkl_name=modelname3a + \".pkl\"\n",
    "joblib.dump(cls, pkl_name)\n",
    "    \n",
    "cls = SVC( probability=True, C=penalty4)\n",
    "cls.fit(a, newy)\n",
    "pkl_name=modelname4a + \".pkl\"\n",
    "joblib.dump(cls, pkl_name)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import load_model\n",
    "#model = load_model('classifier.h5')\n",
    "#model.compile(optimizer = 'sgd', loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tn=5388\n",
    "#fp=8\n",
    "#fn=27\n",
    "#tp=88\n",
    "#sensitivity=tp/(tp + fn)\n",
    "#specificity= tn/(tn + fp)\n",
    "#accuracy=(tp+tn)/(tn+fp + fn +tp)\n",
    "#print(\"accuracy\", accuracy)\n",
    "#print(\"sensitivity\", sensitivity)\n",
    "#print(\"specificity\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#import numpy \n",
    "#newdf=pd.read_csv('gsk3new.txt', sep='\\t', names=['smiles', 'value'])\n",
    "#def predictor(myinput):\n",
    "#    smile=myinput\n",
    "        \n",
    "#    themol=Chem.MolFromSmiles(smile)\n",
    "#    estimator = load_model('classifier.h5')\n",
    "    \n",
    "#    fp = AllChem.GetMorganFingerprintAsBitVect(themol,2,nBits=2048)\n",
    "#    res = numpy.zeros(len(fp),numpy.int32)\n",
    "#    DataStructs.ConvertToNumpyArray(fp,res)\n",
    "#    probas = list(estimator.predict(res.reshape(1,-1))[0])\n",
    "#    print(probas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/patrick/anaconda3/envs/my-rdkit-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/patrick/anaconda3/envs/my-rdkit-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/patrick/anaconda3/envs/my-rdkit-env/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/patrick/anaconda3/envs/my-rdkit-env/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "[6.98245e-06]\n",
      "[0.0057608555]\n",
      "[0.950075]\n",
      "[0.7630735]\n",
      "[0.0009672854]\n",
      "[0.0002627902]\n",
      "[0.009379321]\n",
      "[9.052608e-05]\n",
      "[0.0024876418]\n",
      "[0.0037208511]\n",
      "[0.009120451]\n",
      "[0.0045009707]\n",
      "[0.019255545]\n",
      "[0.015232834]\n",
      "[0.01806407]\n",
      "[0.0033252728]\n",
      "[0.008540479]\n",
      "[1.0284745e-05]\n",
      "[0.00036648734]\n",
      "[5.611415e-06]\n",
      "[4.2615473e-05]\n",
      "[4.2825523e-06]\n",
      "[0.00040753835]\n",
      "[0.00030051664]\n",
      "[4.0693652e-07]\n",
      "[5.948964e-07]\n",
      "[0.00012145685]\n",
      "[1.6462359e-06]\n",
      "[0.0005179627]\n",
      "[0.0]\n",
      "[3.266694e-08]\n",
      "[4.223514e-06]\n",
      "[0.00014644078]\n",
      "[4.6008758e-08]\n",
      "[2.3670049e-08]\n",
      "[6.2193766e-07]\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "#from rdkit import Chem\n",
    "#import numpy as np\n",
    "#from rdkit.Chem import AllChem, DataStructs\n",
    "#validate=newdf['smiles']\n",
    "#testvalidate=newdf['value']\n",
    "#mypreds=[]\n",
    "#for y in validate:\n",
    "#    a=predictor(y)\n",
    "#    mypreds.append(a)\n",
    "#print(mypreds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00056889275]\n",
      "[0.011415197]\n",
      "[0.00022030345]\n",
      "[0.31587937]\n",
      "[0.00012703771]\n",
      "[0.16461712]\n",
      "[0.038860872]\n",
      "[0.00059711566]\n",
      "[0.43126932]\n",
      "[0.012150672]\n"
     ]
    }
   ],
   "source": [
    "#from rdkit import Chem\n",
    "#import numpy as np\n",
    "#from rdkit.Chem import AllChem, DataStructs\n",
    "#newdf=pd.read_csv('testset.txt', names=['smiles'])\n",
    "#validate=newdf['smiles']\n",
    "#for h in validate:\n",
    "#    predictor(h)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
